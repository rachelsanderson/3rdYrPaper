\documentclass[12pt]{article}
\usepackage[margin=0.1in]{geometry}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{natbib}
\usepackage{chngpage}
\usepackage{mathtools,xparse}
\usepackage{graphics}
\usepackage{tikz}
\usepackage{tikz-cd}
\colorlet{shadecolor}{orange!15}
% \definecolor{shadecolor}{rgb}{255,128,0}\
\usepackage{float}
\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{pdflscape}
\usepackage{setspace}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{longtable}
\usepackage{indentfirst}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees}
\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}m{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash}p{#1}}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}m{#1}}

\newcommand{\mc}[3]{\multicolumn{#1}{#2}{#3}}

\tikzset{
  basic/.style  = {draw, text width=2cm, drop shadow, font=\sffamily, rectangle},
  root/.style   = {basic, rounded corners=2pt, thin, align=center,
                   fill=green!30},
  level 2/.style = {basic, rounded corners=6pt, thin,align=center, fill=green!60,
                   text width=8em},
  level 3/.style = {basic, thin, align=left, fill=pink!60, text width=6.5em}
}


\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\newcommand\gamij{\mathbf{\gamma_{ij}}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\params{(p_M, p_{M\ell}, p_{U\ell})}
\newcommand\longparam{(L,n_1,n_2, p_M,p_{M\ell},p_{U\ell})}


%Allows multi-column tables 
\input{./Deadlines/tcilatex}

\setlength{\topmargin}{-0.4in} 
\setlength{\textheight}{8.9in}
\setlength{\parindent}{2em}
% \setstretch{1.25}
\doublespacing
\title{\singlespacing A unified approach to inference with linked data}
\author{Rachel Anderson\thanks{Mailing Address: Department of Economics, Julis Romo Rabinowitz Building,
Princeton, NJ 08544. Email: rachelsa@Princeton.edu.
This project received funding from the NIH (Grant 1 R01 HD077227-01). }}
\date{This Version: \today}

\begin{document}

\maketitle


\begin{abstract}
\singlespacing
\noindent This paper studies what happens when the goal is to estimate a parametric model using observations $(x,y)$, but $x$ and $y$ are observed in distinct datasets with imperfect identifiers.  This setup requires that the researcher must attempt to identify which observations in the $x$- and $y$-datafiles refer to the same individual, prior to performing inference about the joint or conditional distributions of $x$ and $y$.  At a minimum, random errors in this matching step introduce measurement error that must be accounted for in subsequent analyses; however, more serious concerns about sample selection arise when these errors are correlated with unobservables that affect $x$ or $y$.  \end{abstract}

% Using matched data requires estimation techniques that incorporate information from the matching process to improve efficiency and accurately reflect uncertainty. 

\section{Introduction}

This paper studies what happens when the goal is to estimate a parametric model using observations $(x_i,y_i)$, but $x_i$ and $y_i$ are observed in distinct datasets with imperfect identifiers.  This setup requires that the researcher must attempt to identify which observations in the $x$- and $y$-datafiles refer to the same individual, prior to performing inference about the joint or conditional distributions of $x$ and $y$.  At a minimum, random errors in this matching step introduce measurement error that must be accounted for in subsequent analyses; however, more serious concerns about sample selection arise when these errors are correlated with unobservables that affect $x$ or $y$.   

How to perform this match -- representativeness, etc -- has been a focus of recent work by X and Y, but usually the matching step is merely a means to an end.  

Linking multiple data sources without unique IDs is common practice for projects that use historical U.S. data sources prior to the introduction of Social Security Numbers.  For example, \cite{aizer2016} link children listed on Mothers' Pension program welfare applications from 1911-1935 with Social Security Death Master File records from 1965-2012 using individuals' names and dates of birth.  Although the authors are able to match 48 percent of children to a unique death record, 4 percent match to multiple possible records, and 48 percent remain unmatched.\footnote{The authors estimate that at least 32 percent of individuals in the Mothers' Pension program data died before 1965, and therefore should have no match in the 1965-2012 data.}  To avoid dropping the 52 percent of observations with zero or multiple matches, \cite{aizer2016} estimate hazard models using methods from \cite{ahl2019} that allow observations to be associated with multiple, equally likely, outcomes.  

The methods used by \cite{aizer2016} illustrate how inference using linked data requires joint assumptions for the matching and estimation steps.  Under different assumptions, the authors could have generated a ``composite match" equal to the average of the linked observations \citep{bleakley2016}, or constructed bounds on the parameter of interest using different configurations of matched data \citep{nq2015}.   This example also shows how the outputs of the matching process determine which estimation tools are available.  Had the authors used probabilistic record linkage methods to link the data, they could have used the robust OLS estimators from \cite{lahiri05}, or prior-informed imputation for missing records proposed by \cite{Goldstein2012}.   

Although there are a number of recent papers that separately compare the performance of different matching algorithms  \citep{bailey2017, arp2018} and estimation methods for linked data \citep{harron2014}, little is known about the \textit{joint} impact of matching and estimation on the quality of inference with linked data.  This paper fills this gap by comparing how different \textit{combinations} of matching and estimation techniques affect parameter estimates and their confidence intervals in standard econometric models, with the hope of helping researchers choose which methods best suits their projects' needs. 



To be clear, the unit of analysis in this study is a matching/estimation/econometric model combination.   First, I implement several of the most commonly used deterministic and probabilistic record linkage procedures to generate multiple matched versions of the data.  Then, for each matched version of the data, I estimate different types of econometric models, using different estimation methods, and compare the parameter estimates and confidence intervals that these combinations produce.  One iteration of this process is represented in Figure \ref{design}, and Table \ref{methods} contains a comprehensive list of the methods that  are tested. 

\begin{figure}[htbp]
\begin{center}
\caption{Research design}
\includegraphics[width=\textwidth]{./Figures/fig1.pdf}
\label{design}
\end{center}
\end{figure}


\begin{table}[h!]
\centering
\caption{List of methods to be implemented}
\label{methods}
\vspace{10pt}
\small
\makebox[\textwidth]{
\begin{tabular}{llll}
\toprule
\mc{2}{c}{Matching Methods}     &    \mc{2}{c}{Estimation Methods}       \\
\cmidrule(lr){1-2} \cmidrule(lr){3-4}
Deterministic        & Probabilistic      & Uses match weights      & No match weights          \\
\midrule
\cite{abe2012} & \cite{enamorado} & \cite{lahiri05} & \cite{ahl2019}                \\
&   & \cite{Goldstein2012} & \cite{nq2015}        \\
                     &                      & Anderson (2019)         & \cite{bleakley2016} \\
               \bottomrule
\end{tabular}}
\end{table}


At first glance, it's not obvious that the methods studied in this paper should be directly comparable, or whether certain combinations of matching and estimation techniques are possible to implement at the same time; however, I will show that, with a few adjustments, both of these tasks are possible.\footnote{maybe even a theoretical result showing equivalence across classes of matching methods and estimators?}   Specifically, I will show that it is possible to compare the performance of deterministic and probabilistic matching methods by setting thresholds for each method that make the Type I and Type II error rates equal. 

Importantly, the estimation techniques studied in this paper differ according to whether they require estimating the probability that a record pair refers to a match.  When implementing methods that require this extra information in combination with deterministic matching techniques that \textit{do not} output match probabilities, I will treat the matches as being equally likely.  Similarly, if an estimation procedure does not use match probabilities, but a probabilistic matching algorithm outputs them, then the probabilities will be ignored.  






\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{./Figures/gold_data_compare.pdf}
\caption{default}
\label{default}
\end{center}
\end{figure}

In order to illustrate the techniques studied in this paper, I introduce a running example based on synthetic datasets that imitate historical U.S. Census data, yet offer the benefit that each observation's true match is known.  

\textbf{Example 1.}   The ``ground truth" dataset consists of 1000 observations of $(x_{1i}, x_{2i}, y_i, w_i)$, where $x_{1i}$ and $x_{2i}$ are mutually independent, $x_{1i} \overset{i.i.d}{\sim} \text{Bernoulli}(0.5)$, and $x_{2i} \overset{i.i.d}{\sim} \mathcal{N}(0, 2)$.  The value $y_i$ generated by the relationship,
\begin{gather}
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \varepsilon_i, \hspace{10pt} 
\varepsilon_i\  |\  x_{1i}, x_{2i} \overset{i.i.d}{\sim} \mathcal{N}(0, \sigma^2) 
\end{gather}
with $(\beta_0, \beta_1, \beta_2, \sigma^2) = (2, 0.5, 1, 2)$.  Given these parameter values, estimating a correctly specified linear regression model yields an $R^2$ value of approximately 0.50 (see Figure 1(a)).  Each observation is associated with a vector of identifying variables, $w_i$, that consists of a first and last name drawn randomly from a list of first and last names\footnote{The first and last name lists contain 41 and 24 names, respectively, and can be found in the replication files.}, and a random birthday between January 1, 1900, and December 31, 1925, so that the full synthetic dataset resembles the top panel in Figure \ref{sample_dta}.  Note that the number of possible names is smaller than the number of observations to ensure that there are multiple observations with the same name. 
 
 % Massive latex Figure of synthetic dataset -- put in new file ideally
\begin{figure}
\caption{Creation of Synthetic Datasets}
\vspace{5pt}
 \begin{adjustwidth}{-.5in}{-.5in}  
\begin{tikzpicture}
\node (a) at (0,0){
\begin{tabular}{ccccccc}
\toprule
ID &  $y$ & $x_1$ & $x_2$ & First Name & Last Name & Birthday \\
\midrule
1 & $y_1$ & $x_{1,1}$ & $x_{2,1}$ & Tyler & Ashenfelter & 1915-05-13 \\
2 & $y_2$ & $x_{1,2}$ & $x_{2,2}$ & Brandon & Christensen & 1904-06-27 \\
\mc{7}{c}\vdots \\
195 & $y_{195} &$x_{1,195} & $x_{2,195}$ & Samantha & Andersen & 1914-08-18 \\
196 & $y_{196}$ & $x_{1,196}& $x_{2,196}$ & Victoria & Andersen & 1918-11-25\\
\mc{7}{c}\vdots \\ 
1000 & $y_{500}$ & $x_{1,500}$ & $x_{2,500}$ & Vicky & Anderson & 1915-04-14\\
\bottomrule
\end{tabular}};
\\
\vspace{20pt}
\\
\footnotesize{
\node[yshift=-3cm] (b) at (a.south){
\begin{tabular}{ cc }   % top level tables, with 2 rows
$x$-Datafile & $y$-Datafile \\  
% bottom left of the top level table: table 1 
\begin{tabular}{ cccc } 
\toprule
ID & $x$ & Name & Birthday \\
\midrule
2 & ($x_{1,2}, x_{2,2})$ & Branden Christenson & 1905-06-27 \\
\mc{4}{c}\dots \\
195 & ($x_{1,195},x_{2,195}$)& Samantha Anderson & 1914-08-21 \\
198 & ($x_{1,198}, x_{2,198}$)& Jon Smyth & 1918-12-20\\
\mc{4}{c}\dots \\ 
1000 & ($x_{1,1000},x_{2,1000}$) & Vic Andersn & 1915-04-14\\
\bottomrule
\end{tabular} &  % starting bottom right of the top level table
% table 2
\begin{tabular}{ cccc } 
\toprule
ID & $y$ & Name & Birthday \\
\midrule
1 & $y_1$ & Tyler Ashenfelter & 1915-05-13 \\
2 & $y_2$ & Brandon Christensen & 1904-06-27 \\
\mc{4}{c}\dots \\
195 & $y_{1,195}$ & Samantha Anderson & 1914-08-18 \\
\mc{4}{c}\dots \\ 
1000 & $y_{1000}$ & Vicky Anderson & 1915-04-14\\
\bottomrule
\end{tabular} \\
\end{tabular}};
\draw[->, thick](a)--(b);
\end{tikzpicture}
\end{adjustwidth}
\label{sample_dta}
\end{figure}%

Next, I split the ``ground truth" dataset into the $x$- and $y$-datafiles, which contain $(x_1,x_2, w_x)$ and $(y, w_y)$ values respectively.  The $y$-datafile is identical to the ground truth data, except that it excludes the variables $x_1$ and $x_2$.   The $x$-datafile contains values for 400 observations, selected at random from the full dataset. To construct $w_x$, I modify the corresponding $w_y$ by deleting characters (e.g., ``Anderson"  becomes ``Andersn"), exchanging vowels (e.g., ``Rachel" becomes ``Rachal"), or swapping English phonetic equivalents (e.g. ``Ellie" becomes ``Elie").  I also add normally distributed errors to the birth day, month, and year.  The probability of introducing an error to any one element of $w_x$ is set to mimic real-world data \footnote{add a footnote here}.  The $x$- and $y$-datafiles are represented in the bottom panels of Figure \ref{sample_dta}. 


Section 2 introduces the general problem that this paper seeks to address and outlines a common framework for comparing the techniques in Table \ref{methods}.  Sections 3 and 4 describe each of the methods in detail. Section 5 describes the implementation of the methods and the data.  Section 6 contains the results.  Section 7 will be a real empirical application, and Section 8 will conclude.

\section{General problem}
The researcher would like to estimate the linear regression model \begin{equation} y_i = x_i'\beta + \varepsilon_i, \ E[\varepsilon | x_i] = 0  \label{model} \end{equation}
with  $E[\epsilon_i^2] = \sigma^2.$   Instead of observing $(x_i, y_i)$ pairs directly, the researcher observes one dataset with observations $\{x_i, w_i\}_{i=1}^N_x$ and another dataset with observations $\{y_j, w_j\}_{j=1}^{N_y}$.

To estimate ($\ref{model}$) with standard econometric methods, the researcher must identify which of the $x_i$ and $y_j$ refer to the same individuals.   That is, they need to recover the matching function $\varphi: \{1,\dots, N_x\} \to \{1,\dots, N_y\} \cup \varnothing$, where $\varphi(i) = j$ if individual $i$ in the $x$-datafile and individual $j$ in the $y$-datafile refer to the same entity; and $\varphi(i) = \varnothing$ if $i$ does not have a match in $y$-datafile.  If $w_i$ and $w_j$ identify individuals uniquely and do not change over time, then $\varphi(i) = j$ if and only if $w_i = w_j$; otherwise, $\varphi(i) = \varnothing$.  However, if the identifiers are non-unique or prone to errors introduced by the record-generating process, then $\varphi$ needs to be estimated, and inference about $\theta$ needs to be adjusted accordingly.   

In statistics, the task of recovering $\varphi$ is called \textit{record linkage}.  A record linkage procedure consists of a set of decisions about (i) selecting and standardizing the identifiers $w_i$ and $w_j$, (ii) choosing which records to consider as potential matches (especially when $N_1 \times N_2 \times \text{dim}(w_i) $ is large), (iii) defining what patterns of $(w_i,w_j)$ constitute (partial) agreements, and (iv) designating $(i,j)$ pairs as matches.\footnote{Note that this is the author's own definition.  By contrast, \cite{bailey2017} categorize historical linking algorithms (that match observations using name and age only) according to how they treat candidate pairs in the following four categories: (M1) a perfect, unique match in terms of name and age similarity; (M2) a single, similar match that is slightly different in terms of age, name, or both; (M3) many perfect matches, leading to problems with match disambiguation; (M4) multiple similar matches that are slightly different in terms of age, name or both.} Each step of the record linkage process introduces the possibility that a true match is overlooked (Type II error), or that a false match is designated as correct (Type I error), and there is generally a tradeoff between reducing either one of the two \citep{abe2019, harron2018}.

To fix ideas, suppose that $\theta_0$ is the long-run impact of cash transfers on the longevity of children raised in poor families.  The vector $x_i$ includes family and child characteristics as observed in welfare program applications; and the outcomes $y_j$ are constructed by calculating (day of death $-$ day of birth) for all of the observations in a set of death records.  For all $i$ and $j$, the identifiers $w_i$ and $w_j$ include the individual's first and last name, and date of birth.  Additionally, $w_i$ includes $i$'s place of birth; $w_j$ includes $j$'s place of death; and some $w_i$ and $w_j$ contain the individual's middle name or middle initial. 

In this setting, an example of a (deterministic) record linkage procedure consists of: 
\begin{enumerate}
\item[(i)] using a phonetic algorithm to standardize all string variables; 
\item[(ii)] considering as potential matches only $(i,j)$ pairs whose phonetically standardized names begin with the same letter, and whose birth years are within $\pm$2 years;
\item[(iii)] measuring agreements among names using Jaro-Winkler string distances, and weighing disagreements in birth year more than differences in birth month (and more than differences in birth day), 
\item[(iv)] designating as matches all $(i,j)$ pairs with scores calculated using the metrics in (iii) exceeding a pre-specified cutoff; and, if a record $i$ has multiple possible matches $j$ that exceed the cut-off, then choosing the match with the highest score (or picking a random match if there is a tie).  
\end{enumerate}

Another record linkage procedure could be defined using the same rules for steps (i)-(iii), but replace (iv) with a probabilistic matching rule that does not enforce one-to-one matching:
\begin{enumerate}
\item[(iv*)]  use the Expectation-Maximization algorithm to compute ``match weights" for each record pair; then designate as matches all pairs that exceed thresholds that are set to reflect specific tolerances for Type I and Type II error.
\end{enumerate} 

Except in rare cases, the matching function outputted by replacing (iv) with (iv$^*$) will be different.  Whereas the first procedure associates each $x_i$ with at most one matched $y_j$, the second procedure may associate the same $x_i$ with multiple $y_j$ (in technical terms, this implies $\varphi$ is a correspondence).  The former case might use a standard GMM model to estimate $\theta$; while the latter requires methods that associates multiple values of $y_j$ with each $x_i$ \citep{ahl2019}.  This example shows that not only do the estimates of $\theta$ likely depend on the estimates of $\varphi$, but also the \textit{methods} for estimating $\theta$ may be also differ. 

As observed by \cite{bailey2017}, record linkage procedures differ by the set of assumptions that motivate their use.  However, all of the procedures discussed in this paper will be studied under the following, common set of assumptions (with some departures later on):
\begin{enumerate}
\item (De-duplication) Within a given dataset, each observation refers to a distinct entity.  That is, if two observations share the same identifier, they represent two different individuals.
\item (No unobserved sample selection) The observed $x_i$ and $y_j$ are random samples conditional on $w_i$ and $w_j$, respectively.  This means that all individuals with the same identifying information have equal probability of appearing in the sample. 
\item There exists a unique $\theta_0 \in \Theta$ that satisfies the relationship in (\ref{model}), that can be consistently estimated using standard econometric techniques if $\varphi_0$ is known.
\end{enumerate}

The next section discusses in detail the exact record linkage techniques that will be studied, and their motivating assumptions.


%Research on record linkage appears in many fields, such as statistics, computer science, operations research, and epidemiology, under many names, such as data linkage, entity resolution, instance identication, de-duplication, merge/purge processing, and reconciliation.  As such, there are several books devoted to its study \citep{harron_book, christen2012, herzog07}, and dozens of commercial and open source systems software developed for its implementation \citep{kopcke2010}.  Although insights from other fields have been slow to reach economics, recent working papers examine the impact of different record linkage techniques on inference using historic U.S. census data \citep{abe2019, bailey2017}; and the most recent version of the Handbook of Econometrics includes a chapter on the ``Econometrics of Data Combination" \citep{RidderMoffitt}.\footnote{Similar survey papers also exist in fields outside of economics, such as epidemiology and computer science \citep{harron2018, winkler99}.  In fact, that record linkage is studied by many fields makes writing (and reading!) such surveys difficult, because authors are constantly writing the same things For example, \cite{Goldstein2012} prove similar results to those published in \cite{hirukawa2018}.}

%Record linkage techniques are broadly categorized as deterministic or probabilistic\footnote{define them here}; however, every deterministic linkage method has an equivalent probabilistic version \citep{harron2018}.  Furthermore, the first three steps of the record linkage task described above are identical for all procedures.  I begin by discussing these steps, then introduce the PRL framework, and show how it can be used to express deterministic linking rules. 


\section{Record Linkage Methods}
In total, I implement four types of record linkage techniques. I vary probabilistic/deterministic, and allow for unique or multiple matches.  Here I provide an overview of those techniques. 
\subsection{Deterministic}
The deterministic matching algorithm described herein is based upon \cite{abe2012}.  It consists of the following steps
\begin{enumerate}
\item Clean names in $x$ and $y$ datafiles to remove any non-alphabetic characters and account for common mis-spellings and nicknames (e.g., so that Ben and Benjamin would be considered the same name). 
\item Restrict the sample to people who are unique by first and last name, and year and place of birth  % need to decide how to handle date aspect; should i include or not; could add place of birth instead
\item For each record in the $x$-datafile, look for records in the $y$-datafile that match on first name, last name, place of birth, and exact birth year.  At this point there are three possibilities 
\begin{enumerate}
\item If there is a \textit{unique} match, this pair of observations is considered a match.
\item If there are multiple potential matches in the $y$-datafile with the same year of birth, the observation is discarded. 
\item If there are no matches by exact year of birth, the algorithm searches for matches within $\pm$ 1 year of reported birth year, and if this is unsuccessful, it looks for matches within $\pm$ 2 years.  In each of these steps, only unique matches are accepted.  If none of these attempts produces a unique match, the observation is discarded.
\end{enumerate}
\item Step 3 is repeated for each record in the $y$-datafile, after which the intersection of the two matched samples is taken. 
\end{enumerate}

I alter the algorithm slightly so that Step 2 becomes restrict the sample to people who are unique by first and last name, year, place of birth, and $(x_1, x_2)$ values\footnote{This becomes $y$ values when I repeat the algorithm linking $y$ to the $x$-datafile}.  When allowing for multiple matches, I count as matches all record pairs with the same name, and the difference in recorded birth years is within two (or five) years.  That is, I designate all potential matches that arise in Step 3 as matches. 

A quirk of this algorithm is that one person could have a unique exact year match, but then multiple matches with birth years off by 1; this person is included when a unique match is desired.  But if the unique match with zero year difference were not present, then the observation would be dropped. 

\textbf{Example 1 (cont'd).}

\subsection{Probabilistic Record Linkage}
In describing the record linkage techniques implemented in this paper, I use notation from \cite{fellegi69}.  As before, consider two datafiles $X$ and $Y$ that record information from two overlapping sets of individuals or entities.  The files originate from two record-generating processes that may induce errors and missing values, so that identifying which individuals are represented in both $X$ and $Y$ is nontrivial.  I assume that individuals appear at most once in each datafile, so that the goal of record linkage is to identify which records in files $X$ and $Y$ refer to the same entities.

Suppose that files $X$ and $Y$ contain $N_x$ and $N_y$ records, respectively, and without loss of generality that $N_y \geq N_x$.  Denote also the number of entities represented in both files as $N_{xy}$, so that $N_x \geq N_{xy} \geq 0$. 

We say that the set of ordered record pairs $X \times Y$ is the union of two disjoint sets, \textit{matches} $(M)$ and \textit{non-matches} $(U)$:
\begin{align*} M &= \{(i,j): i\in X, j\in Y, i=j\} \\ U &= \{(i,j): i\in X, j\in Y, i\neq j\}\end{align*} 
Hence, the formal goal of record linkage is to identify whether an arbitrary record pair $(i,j)\in X\times Y$ belongs to $M$ or $U$.  

To perform this task, each record pair is evaluated according to $K$ different comparison criteria, which are the result of comparing data fields for records $i$ and $j$.  For example, if a record pair $(i,j)$ represents two individuals, the pair may be evaluated according to whether they share a first name or have the same birthday.  These comparisons are represented by a \textit{comparison vector}, $$\mathbf{\gamma_{ij}}= (\gamma_{ij}^1, \dots, \gamma_{ij}^{k}, \dots, \gamma_{ij}^K)$$  where each comparison field $\gamma_{ij}^{k}$ may be binary-valued, as in ``$i$ and $j$ have the same birthday," or use levels to account for partial agreement between strings (see \citealp{winkler90}, for details).  

\textbf{Example 1 (cont'd).}

The probability of observing a particular configuration of $\gamij$ can be modeled as arising from the mixture distribution:
\begin{equation}
P(\gamij) = P(\gamij | M) p_M + P(\gamij | U) p_U 
\label{mm}
\end{equation}
where $P(\gamma_{ij} | M)$ and $P(\gamma_{ij} | U)$ are the probabilities of observing the pattern $\gamma_{ij}$ conditional on the record pair $(i,j)$ belonging to $M$ or $U$, respectively.  The proportions $p_M$ and $p_U = 1-p_M$ are the marginal probabilities of observing a matched or unmatched pair.  Applying Bayes' Rule, we obtain the probability of $(i,j) \in M$ conditional on observing $\gamij$,
\begin{equation} P(M | \gamij) = \frac{p_M P(\gamij | M)}{P(\gamij)} \label{bayes} \end{equation}
so that if we can estimate the variables in (\ref{mm}), we can estimate the probability that any two records refer to the same entity in (\ref{bayes}).   These probabilities can then be used to designate pairs as matches, and to estimate the false positive rates associated with any potential match configuration.

Let $\mathbf{\Gamma} \equiv \{\mathbf{\gamma_{ij}}: \  (i,j) \in X\times Y\}$ denote the set of comparison vectors for all records pairs $(i,j) \in X\times Y$.   Note that $\mathbf{\Gamma}$ contains potentially $N_x \times N_y$ elements, so that calculating $\mathbf{\Gamma}$ may be computationally expensive when $X$ or $Y$ is large.  In practice, researchers partition $X\times Y$ into ``blocks," such that only records belonging to the same block are attempted to be linked, and records belonging to different blocks are assumed to be non-matches.   Importantly, the blocking variables should be recorded without error, and sometimes there are none available. 


\textbf{Example 1 (cont'd).} This paper assumes that no blocking is used; or, alternatively, that records are already divided into blocks that can be analyzed independently using the methods outlined below.  

Another important simplifying assumption is that of conditional independence.  While in principle we can model, 
\begin{align*} (\gamma_{ij}^1, \dots, \gamma_{ij}^K) \  |\  M &\sim \text{Dirichlet}(\mathbf{\delta_M})\\
 (\gamma_{ij}^1, \dots, \gamma_{ij}^K) \  |\  U &\sim \text{Dirichlet}(\mathbf{\delta_U}) \end{align*}
there are $2^{K}-1$ possible configurations of each $\gamij$, so that $\mathbf{\delta_M}$ and $\mathbf{\delta_U}$ may be high-dimensional.  Instead, if the comparison fields are structured so that the $\gamma_{ij}^{k}$ are independent across $k$ conditional on match status, then,
 \begin{equation} 
 P(\gamma_{ij} | C) = \prod_{k=1}^K P(\gamma_{ij}^{k} | C)^{\gamma_{ij}^{k}}(1-Pr(\gamma_{ij}^{k} | C))^{1-\gamma_{ij}^{k}} \hspace{20pt} C\in \{M, U\} 
 \label{eq:condInd}
 \end{equation}
and the number of parameters used to describe each mixture class is reduced to $K$.  This assumption can be relaxed using log-linear models, but for now I assume conditional independence to ease computation.  

\textbf{Example 1 (cont'd)}.  Errors are constructed to satisfy this assumption

\section{Estimation Methods}
\subsection{OLS Bias Correction}
\cite{lahiri05} take as input two files are linked by a computerized record linkage technique (CRL).  The true data pairs $(x_i, y_i)$ are not observable; instead, the CRL produces pairs $(x_i, z_i)$ in which $z_i$ may or may not correspond to $y_i$.  The (true) regression model is:
\begin{gather*}y_i = x_i'\beta + \epsilon_i,\ E[\epsilon_i] = 0,\\ \text{var}(\epsilon_i) = \sigma^2,\ \text{cov}(\epsilon_i \epsilon_j) = 0 \end{gather*}
but the researcher estimates this model with $z_i$ as the dependent variable, where $$z_i = \begin{cases} y_i & \text{with probability $q_{ii}$} \\ y_j & \text{with probability $q_{ij}$ for $j\neq i,\ j = 1,\dots,n $} \end{cases}$$ 
and $\sum_{j=1}^n q_{ij} = 1, \ i=1,\dots, n$.  Define $\mathbf{q_i} = (q_{i1}, \dots, q_{in})'$.  The naive least squares estimator of $\beta$, which ignores mismatch errors, is given by:
$$\hat{\mathbf{\beta}}_N = (\mathbf{X'X})^{-1} \mathbf{X'z} $$ 
An alternative to this naive estimator is one that minimizes the sum of absolute deviations, which decrease the influence of outliers and hence should decrease the impact of erroneously paired predictor and response values.  

Note that $E(z_i) = \mathbf{w_i'\beta}$, with $\mathbf{w_i = q_i'X_i} = \sum_{j=1}^n q_{ij} x_j' $, and so the bias of $\hat{\mathbf{\beta}}_N $ is given by
$$\text{bias}(\hat{\beta}_N) = [(X'X)^{-1} X'QX - I] \beta $$ 
Hence, if $Q = I$, then $\hat{\beta}_N$ is unbiased.  This is equivalent to giving all potential matches the same weight (i.e. treating all matches as equally likely to be correct), as discussed in \cite{ahl2019}.  

In order to reduce the bias of $\hat{\beta}_N$, Scheuren and Winkler (1993) %add citation
observed that 
$$\text{bias} (\hat{\beta}_N | y) = E[(\hat{\beta}_N - \beta) | y ] = (X'X)^{-1} X'B,$$ 
where $B = (B_1, \dots, B_n)'$ and $B_i = (q_{ii}-1)y_i + \sum_{j\neq i } q_{ij} y_j = \mathbf{q_i'y} - y_i$, which is the difference between a weighted average of responses from all observations and the actual response $y_i$.  Thus, if an estimator $\hat{B}$ is available, the SW estimator is given by:
$$ \hat{\beta}_{SW} = \hat{\beta}_N - (X'X)^{-1} X' \hat{B}$$ 
SW give a truncated estimator of $B_i$ using the first and second highest elements of the vector $q_i$, $\hat{B}_i^{TR} = (q_{ij_1} - 1) z_{j_1} + q_{ij_2} z_{j_2}$, that can also be written more generally for an arbitrary number of elements of $q_i$.  This means that $\hat{\beta}_{SW}$ is not unbiased, but, if the probability is high that the best candidate link is the true link, then the truncation might produce a very small bias. 

Using $E(z) = W\beta$, \cite{lahiri05} propose an exactly unbiased estimator of $\beta$:
$$ \hat{\beta}_U = (W'W)^{-1} W'z$$ 
where $W = (w_1, \dots, w_N)'$, $w_i = q_i'X_i$ as above.  They suggest using a truncated version of $w_i$, $w_i^{TR} = q_{ij_1} x_{j_1} + q_{ij_2} x_{j_2}$.   \cite{lahiri05} use estimates of $Q$ obtained from applying the Fellegi-Sunter/EM procedure, and observe that replacing $Q$ with $\hat{Q}$ yields unbiased estimates of $\beta$ whenever $\hat{Q}$ can be assumed to be independent of $z$.  They argue that this is expected to be true in most applications, because the distribution of matching variables (e.g. first and last name, age), which determines the distribution of $\hat{Q}$, is usually independent of the response variable $y$ (e.g. income), and hence of $z$.  

Importantly, this assumption does not hold in some economics applications, such as the racial ``passing" example from \cite{nq2015}. 

\cite{lahiri05} conclude that in simulations, least median regression is not sufficient to guard against matching errors, whereas the method of SW (1003) made a useful adjustment.  Their method performed well across a range of situations,a nd the bootstrap procedure is useful for reflecting uncertainty due to matching.  


\subsection{Multiple match WLS}

subsection{\cite{ahl2019}}
\subsection{\cite{nq2015}}
\cite{nq2015} study racial passing by linking individual U.S. census records to determine whether an individual's recorded race changed from one census to the next.  To achieve higher match rates than those of previous studies\footnote{The authors match 61-67 percent of individuals.   ABE (2012), Hornbeck and Naidu (2014), Long and Ferrie (2013), Mill and Stein (2012)
have match rates around 30, 24, 22, 11-34 percent respectively}, the authors develop methods for including individuals with multiple potential matches.  These methods include selecting one match at random, and selecting the match that produces an upper/lower bound for estimating the ``passing" rate.  

\cite{nq2015} also use the unmatched individuals from their data to calculate absolute bounds for the population passing rates.  For a given algorithm, the absolute upper bound is obtained by using the ``upper bound" configuration of data, combined with assuming that all unmatched individuals passed.  The lower bound is obtained in the same way, assuming that none of the excluded individuals passed. 

\subsection{\cite{bleakley2016}}

\cite{bleakley2016} assign equal probability of winning (matched variable equal to $1/n$) to all $n$ individuals matched to the same winner.  The goal is to estimate the treatment effect of winning a parcel in the lottery by comparing mean outcomes for winners and losers in a simple bivariate regression with a dummy varialbe for winning a parcel on the right-hand side.   Here, winning the lottery is coded as $0$ or $1/n$, where $n$ is the number of matches for person $i$. [Think about how this compares to ahl method]

\subsection{\cite{lahiri05}}

\subsection{\cite{Goldstein2012}}


Prior-informed imputation (PII), proposed in \cite{Goldstein2012}, aims to select the
correct value for variables of interest, rather than accepting a single complete record as a link.  Information from match probabilities in candidate linking records (the prior) is combined with information
from unequivocally linked records.  

The method is more or less as follows: use the usual FS/EM procedure to estimate probabilities of candidate pairs referring to a match.  The result is each record $i$ is associated with $\{y_{ij}\}$ and probabilities $p_{ij}$.  Assume, wlog, that all variables follow a joint multivariate normal distribution\footnote{If this is not the case for some of the observed variables, then a joint MVN distribution can be obtained using the latent MVN trick (for categorical variables, imputed values are back-transformed to their original scales.} A lower threshold can be chosen so that records with probabilities less than a threshold are ignored.  In practice, they recommend ignoring records that have no match on any matching variable; regard these records as having missing variables and use standard MI.

Denote distribution of variables in linking datafile A ($y$ in ahl framework), conditional on variables in the file of interest B that they are linking to, by $f(Y^{A|B})$.   Conditioning is on responses and any covariates in the imputation model, includes variables from A that are treated as auxiliary predictor variables in the imputation model.  This conditional distribution is also multivariate normal.  

For each record $i$, we compute a modified prior probability $\pi_{ij}$ which is the likelihood component $f(Y^{A|B})$ multiplied by the prior $p_{ij}$, so that $\pi_{ij} \propto f(Y^{A|B}) p_{ij}$.  The normalized set $\pi_i$ comprises the modified probability distribution for each $i$ record in A.  

Set a lower threshold for accepting a record as a true link, and if any records exceeds this, we choose that with the largest probability.  If no record exceeds the threshold, then we regard the data values as missing and use standard multiple imputation.  ``The largest gain can be expected to arise
when the probability of a link is associated with the values of the variables to be transferred. When the
MAR assumption discussed earlier holds, then given a high enough threshold, the proposed procedure
will produce unbiased estimates" Incorporating the likelihood component can be expected to lead more often to the correct record exceeding a given threshold. 

For missing observation: Assume we have an estimate of mortality rate of individuals in A, $\pi_d$.  If a proportion of the A file $\pi_m < \pi_d$ are unequivocally matched, then the probability that a randomly chosen remaining record in B is not a death from the A file is $\pi_r = 1 - (\pi_d - \pi_m)$.  Therefore multiply $p_i$ by $1-\pi_r$ and add an extra pseudo-record with probability $\pi_r$ with an associated code for a surviving patient. 



\subsection{This paper}


\section{Data and Implementation}

\section{Results}



\newpage
\singlespacing
\bibliography{./Deadlines/proposal_bib} 
\bibliographystyle{IEEEtranN}

\end{document}