\documentclass[12pt]{article}
\usepackage[margin=0.1in]{geometry}
\usepackage{xcolor}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{pdflscape}
\usepackage{natbib}
\usepackage{mathtools,xparse}
\usepackage{graphics}
\colorlet{shadecolor}{orange!15}
% \definecolor{shadecolor}{rgb}{255,128,0}\
\usepackage{float}
\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{longtable}
\usepackage{indentfirst}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees}

\tikzset{
  basic/.style  = {draw, text width=2cm, drop shadow, font=\sffamily, rectangle},
  root/.style   = {basic, rounded corners=2pt, thin, align=center,
                   fill=green!30},
  level 2/.style = {basic, rounded corners=6pt, thin,align=center, fill=green!60,
                   text width=8em},
  level 3/.style = {basic, thin, align=left, fill=pink!60, text width=6.5em}
}

%Allows multi-column tables 
\input{../tcilatex}

\setlength{\topmargin}{-0.4in} 
\setlength{\textheight}{8.9in}
\setlength{\parindent}{2em}
% \setstretch{1.25}
\title{\singlespacing Annotated bibliography}
\author{\singlespacing \vspace{-50pt} Rachel Anderson\thanks{Mailing Address: Department of Economics, Julis Romo Rabinowitz Building, Princeton, NJ 08544. Email: rachelsa@princeton.edu. This project received funding from the NIH (Grant 1 R01 HD077227-01). }}
\date{\vspace{-40pt} This Version: \today}

\begin{document}
\doublespacing
\maketitle

\section{Challenges in historical record linkage}
Record linkage in economics is most often studied by those who use historical U.S. data prior to the introduction of social security numbers \citep{abe2019,aizer2016,ferrie96}.  These data tend to lack unique identifiers, so that individuals are primarily identified by their first and last name, and age\footnote{Sometimes middle initial, birthplace, parent's name and birthplace, exact birthdate etc. are available; but data quality vary by application.}, which are rarely unique within a given population.   The non-uniqueness problem is exacerbated by typographical error, caused by low literacy rates and regional variations in names, as well as the record digitization process itself which introduces yet another possible source of error.    

For example, \cite{nq2015} remark that an illiterate individual from Louisiana with the surname of Thibideaux, who chooses to move to another state, would likely have his name spelled phonetically as Tibido.  Goeken et al. (2017) document that in two enumerations of St. Louis in the 1880 Census, nearly 46 percent of first names are not exact matches, and the Early Indicators project notes that 11.5 percent of individuals in the Oldest Old sample have a shorter first name in pension records than in the original Civil War enlistment records (Costa et al. 2017). 

Researchers use a variety of techniques -- phonetic algorithms, string comparators, and probabilistic record linkage -- to link records from different files despite errors that arise in the record generating process.  

Compared to other fields where record linkage is a research goal in itself, record linkage in economics is seen as prerequisite to answering economic questions.  Economists contribute to the record linkage literature by focusing on historic data; with care for the impact it has on subsequent inference.   

This is problem bc  Neter, Maynes, and Ramanathan (1965): small mismatch errors in finite population sampling can lead to a substantial bias in estimating the relationship between response errors and true values

\section{Matching Methods}

\subsection{Survey Papers}

Record linkage is such an intricate and frequent challenge that there are several books devoted to its study, and dozens of commercial and freely available software devoted to the task \citep{christen2012, herzog07}.  The research and open source systems include those developed by census bureaus, computer scientists, epidemiologists, statisticians, political scientists, and -- only more recently -- economists.  A recent survey by \cite{kopcke2010} evaluates eleven such systems.  

The challenge in studying record linkage and its effects on inference, are magnified by the fact that research on the topic occurs in disparate fields.  Goldstein, Harron and Wade (2012) prove the same result that is published six years later in \cite{hirukawa2018}. 

The father of modern record linkage, Winkler in \cite{christen2012} says: ``Although individuals have introduced alternative classification methods based on Support Vector Machines, decision trees and other
methods from machine learning, no method has consistently outperformed methods
based on the Fellegi?Sunter model, particularly with large day-to-day applications
with tens of millions of records"

\cite{bailey2017} review literature on historical record linkage in US and examines performance of automated record linkage algorithms with two high-quality historical datasets and one synthetic ground truth.  They conclude that no method consistently produces representative samples; machine linking has high number of false links and may introduce bias into analyses.  

\cite{arp2018} have guide for researchers in the choice of which variables to use for linking, how to estimate probabilities, and then choose which records to use in the analysis.  Created R code and stata command to implement the method

\cite{abe2019} evaluate different automated methods for record linkage, specifically deterministic (like Ferrie and ABE papers), machine learning Feigenbaum approach, and the AMP approach with the EM algorithm.  Document a frontier between type I and type II errors; cost of low false positive rates comes at cost of designating relatively fewer (true) matches.  Humans typically match more at a cost of more false positives.  They study how different linking methods affect inference -- sensitivity of regression estimates to the choice of linking algorithm.  They find that the parameter estimates are stable across linking methods.  Find effect of matching algorithm on inference is small. 

\subsection{Surveys on Estimation Methods}
`` Evaluating bias due to data linkage error in
electronic healthcare records" -- compares the highest probability match (nearest neighbor) with prior-induced imputation

\subsection{Matching Methods}

\cite{bailey2017} categorize historical linking algorithms (that match observations using name and age only) according to how they treat candidate pairs in the following four categories:
\begin{itemize}
\item M1: A perfect, unique match in terms of name and age similarity 
\item M2: A single, similar match that is slightly different in terms of age, name, or both
\item M3: Many perfect matches, leading to problems with match disambiguation
\item M4:  Multiple similar matches that are slightly different in terms of age, name or both 
\end{itemize}
Historical linking algorithms generally treat M1 cases as matches, but differ in how they treat M2, M3, and M4 candidate pairs.   Generally, differences in M2 are solved deterministically by setting fixed-year band tolerances for matches, and probabilistically by estimating weights for the relative importance of age vs. name agreement.  Multiple matches in M3/M4 are ignored, picked at random, given equal weights, or given weights proportional to the probability of being the true match.  Table X below provides an overview of methods in literature based on these dimensions. 

[Insert table here] 
Table includes 

Talk also about how to evaluate these matching methods -- what is desirable? How to estimate error rates ex post! 

\begin{itemize}
\item Ferrie 1996, Abramitzky, BOustan and Eriksson (2012 2014 2017) Deterministic.  Conservative methods require no other potential match with same name within a 5-year band , Nix and Qian
\item Semi-automated Feigenbaum, Ruggles et al 

\end{itemize}


\section{Estimation Methods}

\subsection{\cite{lahiri05}}
\cite{lahiri05} take as input two files are linked by a computerized record linkage technique (CRL).  The true data pairs $(x_i, y_i)$ are not observable; instead, the CRL produces pairs $(x_i, z_i)$ in which $z_i$ may or may not correspond to $y_i$.  The (true) regression model is:
\begin{gather*}y_i = x_i'\beta + \epsilon_i,\ E[\epsilon_i] = 0,\\ \text{var}(\epsilon_i) = \sigma^2,\ \text{cov}(\epsilon_i \epsilon_j) = 0 \end{gather*}
but the researcher estimates this model with $z_i$ as the dependent variable, where $$z_i = \begin{cases} y_i & \text{with probability $q_{ii}$} \\ y_j & \text{with probability $q_{ij}$ for $j\neq i,\ j = 1,\dots,n $} \end{cases}$$ 
and $\sum_{j=1}^n q_{ij} = 1, \ i=1,\dots, n$.  Define $\mathbf{q_i} = (q_{i1}, \dots, q_{in})'$.  The naive least squares estimator of $\beta$, which ignores mismatch errors, is given by:
$$\hat{\mathbf{\beta}}_N = (\mathbf{X'X})^{-1} \mathbf{X'z} $$ 
An alternative to this naive estimator is one that minimizes the sum of absolute deviations, which decrease the influence of outliers and hence should decrease the impact of erroneously paired predictor and response values.  

Note that $E(z_i) = \mathbf{w_i'\beta}$, with $\mathbf{w_i = q_i'X_i} = \sum_{j=1}^n q_{ij} x_j' $, and so the bias of $\hat{\mathbf{\beta}}_N $ is given by
$$\text{bias}(\hat{\beta}_N) = [(X'X)^{-1} X'QX - I] \beta $$ 
Hence, if $Q = I$, then $\hat{\beta}_N$ is unbiased.  This is equivalent to giving all potential matches the same weight (i.e. treating all matches as equally likely to be correct), as discussed in \cite{ahl2019}.  

In order to reduce the bias of $\hat{\beta}_N$, Scheuren and Winkler (1993) %add citation
observed that 
$$\text{bias} (\hat{\beta}_N | y) = E[(\hat{\beta}_N - \beta) | y ] = (X'X)^{-1} X'B,$$ 
where $B = (B_1, \dots, B_n)'$ and $B_i = (q_{ii}-1)y_i + \sum_{j\neq i } q_{ij} y_j = \mathbf{q_i'y} - y_i$, which is the difference between a weighted average of responses from all observations and the actual response $y_i$.  Thus, if an estimator $\hat{B}$ is available, the SW estimator is given by:
$$ \hat{\beta}_{SW} = \hat{\beta}_N - (X'X)^{-1} X' \hat{B}$$ 
SW give a truncated estimator of $B_i$ using the first and second highest elements of the vector $q_i$, $\hat{B}_i^{TR} = (q_{ij_1} - 1) z_{j_1} + q_{ij_2} z_{j_2}$, that can also be written more generally for an arbitrary number of elements of $q_i$.  This means that $\hat{\beta}_{SW}$ is not unbiased, but, if the probability is high that the best candidate link is the true link, then the truncation might produce a very small bias. 

Using $E(z) = W\beta$, \cite{lahiri05} propose an exactly unbiased estimator of $\beta$:
$$ \hat{\beta}_U = (W'W)^{-1} W'z$$ 
where $W = (w_1, \dots, w_N)'$, $w_i = q_i'X_i$ as above.  They suggest using a truncated version of $w_i$, $w_i^{TR} = q_{ij_1} x_{j_1} + q_{ij_2} x_{j_2}$.   \cite{lahiri05} use estimates of $Q$ obtained from applying the Fellegi-Sunter/EM procedure, and observe that replacing $Q$ with $\hat{Q}$ yields unbiased estimates of $\beta$ whenever $\hat{Q}$ can be assumed to be independent of $z$.  They argue that this is expected to be true in most applications, because the distribution of matching variables (e.g. first and last name, age), which determines the distribution of $\hat{Q}$, is usually independent of the response variable $y$ (e.g. income), and hence of $z$.  

Importantly, this assumption does not hold in some economics applications, such as the racial ``passing" example from \cite{nq2015}. 

\cite{lahiri05} conclude that in simulations, least median regression is not sufficient to guard against matching errors, whereas the method of SW (1003) made a useful adjustment.  Their method performed well across a range of situations,a nd the bootstrap procedure is useful for reflecting uncertainty due to matching.  

\subsection{\cite{nq2015}}

%TO DO: put their bounds idea into math! 

\cite{nq2015} study racial passing by linking individual U.S. census records to determine whether an individual's recorded race changed from one census to the next.  To achieve higher match rates than those of previous studies\footnote{The authors match 61-67 percent of individuals.   ABE (2012), Hornbeck and Naidu (2014), Long and Ferrie (2013), Mill and Stein (2012)
have match rates around 30, 24, 22, 11-34 percent respectively}, the authors develop methods for including individuals with multiple potential matches.  These methods include selecting one match at random, and selecting the match that produces an upper/lower bound for estimating the ``passing" rate.  

\cite{nq2015} also use the unmatched individuals from their data to calculate absolute bounds for the population passing rates.  For a given algorithm, the absolute upper bound is obtained by using the ``upper bound" configuration of data, combined with assuming that all unmatched individuals passed.  The lower bound is obtained in the same way, assuming that none of the excluded individuals passed. 

\cite{nq2015} argue that increasing the match rate improves the bounds around any true population statistic, even though their methods introduce random measurement error in the estimand.  Below is a visual of their complex blocking strategy.

\begin{figure}[htbp]
\begin{center}
\caption{\cite{nq2015} blocking strategy}
\includegraphics[width=\textwidth]{nq_method.png}
\label{default}
\end{center}
\end{figure}

\subsection{Prior Informed Imputation \citep{Goldstein2012}}

Use the usual FS/EM procedure to estimate probabilities of candidate pairs referring to a match.  The result is each record $i$ is associated with $\{y_{ij}\}$ and probabilities $p_{ij}$.  Assume, wlog, that all variables follow a joint multivariate normal distribution\footnote{If this is not the case for some of the observed variables, then a joint MVN distribution can be obtained using the latent MVN trick (for categorical variables, imputed values are back-transformed to their original scales.} A lower threshold can be chosen so that records with probabilities less than a threshold are ignored.  In practice, they recommend ignoring records that have no match on any matching variable; regard these records as having missing variables and use standard MI.

Denote distribution of variables in linking datafile A ($y$ in ahl framework), conditional on variables in the file of interest B that they are linking to, by $f(Y^{A|B})$.   Conditioning is on responses and any covariates in the imputation model, includes variables from A that are treated as auxiliary predictor variables in the imputation model.  This conditional distribution is also multivariate normal.  

For each record $i$, we compute a modified prior probability, which is the likelihood component $f(Y^{A|B})$ multiplied by the prior $p_{ij}$, so that $\pi_{ij} \propto f(Y^{A|B}) p_{ij}$. 

\subsection{\cite{bleakley2016}}

\cite{bleakley2016} assign equal probability of winning (matched variable equal to $1/n$) to all $n$ individuals matched to the same winner.  The goal is to estimate the treatment effect of winning a parcel in the lottery by comparing mean outcomes for winners and losers in a simple bivariate regression with a dummy varialbe for winning a parcel on the right-hand side.   Here, winning the lottery is coded as $0$ or $1/n$, where $n$ is the number of matches for person $i$. [Think about how this compares to ahl method]

\subsection{Other ideas to include?}
\cite{hirukawa2018} give OLS methods for nearest neighbor matched samples.  Not explicitly the probabilistic record linkage framework, although the authors use this term. Could apply their methods for an appropriately defined metric for matching variables (i.e. Jaro-Winkler string distance), and use a nearest neighbor matching rule to assign 1-1 matches (although some $y$ may get mapped to multiple $x$. 



% CHECK THESE OUT !!!!
 Scheuren and Winkler (1993): propose method for adjusting for bias of mismatch error in OLS
 SW (1997, 1991): iterative procedure that modifies regression and matching results for apparent outliers 
 Enamorado procedures
 Survey paper from handbook of econometrics
 
 "However, the analytic estimates of precision in Lahiri
and Larsen (2005) are poor for 1-1 probabilistic linkage (Chipperfield and Chambers
2015)"   As a quality measure, Christen (2012) suggests precision, which is the proportion of
links that are true matches. Winglee et al. (2005) use a simulation-based approach,
Simrate to estimate linkage quality. Their method uses the observed distribution of
data in matched and non-matched pairs to generate a large simulated set of record 

FROM https://arxiv.org/pdf/1901.04779.pdf
 


\begin{figure}[htbp]
\begin{center}
\includegraphics[width=\textwidth]{methods_table.pdf}
\caption{default}
\label{default}
\end{center}
\end{figure}

\section*{Simulation Idea}
Could use only simulation data, with variety of possible biases, motivated by the applications above.  For example, motivated by  N-Q, introduce error with geographical relocation.  Then test which techniques are robust to these types of sample selection/error.   

I will compare estimates of Type I/ Type II error to ACTUAL Type I/ Type II error rates, and say that authors need to estimate their errors!! Not just report the match rate.   Use estimates from Chipperfield (2018) maybe https://onlinelibrary.wiley.com/doi/epdf/10.1111/insr.12246

\newpage

Matching surveys
\begin{itemize}
\item Books: \cite{christen2012, herzog07}
\item Free software: \cite{kopcke2010}
\item Economics world: \cite{bailey2017}, \cite{abe2019} 
\item Winkler? 
\end{itemize}

Estimation papers
\begin{itemize}
\end{itemize}

\bibliography{../proposal_bib} 
\bibliographystyle{IEEEtranN}
\end{document}